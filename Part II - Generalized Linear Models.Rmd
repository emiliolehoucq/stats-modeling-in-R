---
title: "Part II: Generalized Linear Models"
output:
  pdf_document: default
  html_document: default
---

# Load Packages 

Again, we must load the packages that will be used in the first part of this workshop.


```{r, message=FALSE, warning=FALSE }
library(pastecs, quietly = TRUE)
library(lm.beta,  quietly = TRUE)
library(lmtest,  quietly = TRUE)
library(foreign,  quietly = TRUE)
library(lattice,  quietly = TRUE)
library(lme4,  quietly = TRUE)
library(nlme,  quietly = TRUE)
library(survival,  quietly = TRUE)
library(dplyr,  quietly = TRUE)
library(ggfortify,  quietly = TRUE)
library(survminer,  quietly = TRUE)
library(rms,  quietly = TRUE)
library(MASS, quietly = TRUE)
library(pscl, quietly = TRUE)

```

# Generalized linear models

A generalized linear model (GLM) has three:

- a random component with mean $\mu$. Generally, the random component is the response variable $Y_i.$
- a systematic component, $\eta_i$, that relates the relates the explanatory variables,
$$\eta_i = \sum_{j=i}^{n} \beta_j x_{ij}$$
- a link function that relates the mean of the random to the systematic component
$$g(\mu) = \eta_i$$

# Logistic regression

Logistic regression is a GLM used the model binary (0 or 1) data. The response variable must be binary and is assumed to follow a bernoulli distribution. 

That said, logistic regression has the following properties:

-  a response binary variable, $Y_i$, that follows a bernoulli distribution with mean $\pi_i$.
- a systematic component, $\eta_i$, that relates the relates the explanatory variables,
$$\eta_i = \sum_{j=1}^{n} \beta_j x_{ij}$$
- a link function that relates the mean of the random to the systematic component
$$\log\left(\frac{\pi_i}{1-\pi_i}\right) = \sum_{j=i}^{n} \beta_j x_{ij}.$$ $\log\left(\frac{\pi_i}{1-\pi_i}\right)$ is known as the log odds.

## Data 

Using the iris data, we create binary data. We add the column `Sepal.Width_binary` to iris. If the `Sepal.Width` is greater than the median then the associated value in `Sepal.Width_binary` is 1. Otherwise, `Sepal.Width_binary` is 0.

```{r}
data <- iris
data$Sepal.Width_binary <- ifelse(data$Sepal.Width >= median(data$Sepal.Width), 1, 0)
```

## Logistic Regression with only the constant term

Fitting only a constant term, the systematic component is
$$\eta_i = \beta_0$$
```{r}
logit <- glm(Sepal.Width_binary ~ 1, data = data, family = "binomial")
summary(logit)
```

```{r}
p_avg <- mean(data$Sepal.Width_binary)
log_odds_avg <- log(p_avg/(1-p_avg))
print(log_odds_avg)
```

## Logistic Regression with Species  

Fitting the species term, the systematic component is
$$\eta_i = 1 + \beta_2 X_{1i} + \beta_3 X_{2i}.$$

where 
$$  X_{1i} = \begin{cases} 1 & \text{if } i\text{th data point is versicolor}\\ 0 &  \text{otherwise}\end{cases}, \, X_{2i} = \begin{cases} 1 & \text{if } i \text{th data point is virginica}\\ 0 & \text{otherwise}\end{cases}$$


```{r}
logit <- glm(Sepal.Width_binary ~ as.factor(Species), data = data, family = "binomial")
summary(logit)
```

Let's compare the results to the average log odds of each Species group
```{r}
log_odds_avg_fun <- function(data){
  p_avg <- mean(data)
  log_odds_avg <- log(p_avg/(1-p_avg))
  return(log_odds_avg)
}

tapply(data$Sepal.Width_binary,
       data$Species, log_odds_avg_fun)
```

The intercept corresponds to the average log odds of setosa as we would expect. However, the other coefficients do not correspond to the average log odds of the other species. Why?

From the formula, $\eta_i = 1 + \beta_2 X_{2i} + \beta_3 X_{3i}$, the log odds of versicolor actually corresponds to $1+\beta_2$. The log odds of versicolor actually corresponds to $1+\beta_3$.

```{r}
coefficients<-unname(coef(logit))
print(c(coefficients[1],coefficients[1]+coefficients[2],
        coefficients[1]+coefficients[3]))
```
## Logistic Regression with continuous variable

*add discussion here *


## Logistic Regression with continuous variable, Sepal.Length

Fitting the species term, the systematic component is

$$\eta_i =  \beta_3 X_{1i}.$$

where  $X_{1i} = \text{Sepal.Length of the }i\text{th  data point}.$

```{r}
logit <- glm(Sepal.Width_binary ~ Sepal.Length, 
             data = data, family = "binomial")
summary(logit)
```


```{r}
plot(Sepal.Width_binary~Sepal.Length, data=data)
points(data$Sepal.Length[order(data$Sepal.Length)],
       logit$fitted[order(data$Sepal.Length)],  col="red")
title(main="Data with Fitted Logistic Regression Line")
```

## Logistic Regression with Species and Sepal.Length

Fitting the species term, the systematic component is
$$\eta_i = 1 + \beta_2 X_{1i} + \beta_3 X_{2i} + \beta_3 X_{3i}.$$

where 
$$  X_{1i} = \begin{cases} 1 & \text{if } i\text{th data point is versicolor}\\ 0 &  \text{otherwise}\end{cases}, \, X_{2i} = \begin{cases} 1 & \text{if } i \text{th data point is virginica}\\ 0 & \text{otherwise}\end{cases}$$ and $X_{3i} = \text{Sepal.Length of the }i\text{th  data point}.$

Fitting the logistic model accordingly,

```{r}
logit <- glm(Sepal.Width_binary ~ Species +Sepal.Length, 
             data = data, family = "binomial")
summary(logit)
```
Plot the results for each species, we get that
```{r}
plot(data[data$Species == "setosa", ]$Sepal.Length, 
     data[data$Species == "setosa", ]$Sepal.Width_binary, 
     xlim=as.matrix(range(data$Sepal.Length)),
     xlab = 'Sepal Length',  ylab= 'Sepal Width binary',
     main= 'Scatter plot of sepal length vs sepal width')

points(data$Sepal.Length[data$Species == "setosa"],
       logit$fitted[data$Species == "setosa"],  pch=15,
       col="red")
```

```{r}
plot(data[data$Species == "versicolor", ]$Sepal.Length,
     data[data$Species == "versicolor", ]$Sepal.Width_binary,
     xlim=as.matrix(range(data$Sepal.Length)),
     xlab = 'Sepal Length',  ylab= 'Sepal Width binary',
     main= 'Scatter plot of sepal length vs sepal width')

points(data$Sepal.Length[data$Species == "versicolor"],
       logit$fitted[data$Species == "versicolor"],  pch=15,
       col="yellow")
```

```{r}
plot(data[data$Species == "virginica", ]$Sepal.Length,
     data[data$Species == "virginica", ]$Sepal.Width_binary,
     xlim=as.matrix(range(data$Sepal.Length)),
     xlab = 'Sepal Length',  ylab= 'Sepal Width binary',
     main= 'Scatter plot of sepal length vs sepal width')

points(data$Sepal.Length[data$Species == "virginica"],
       logit$fitted[data$Species == "virginica"],  pch=15,
       col="blue")
```

## Goodness of Fit

### Deviance

 For general linear models, we use *deviance* to the compare to two different models. Deviance is the difference in log likelihood of the models multipled by 2.
 
### Saturated Model
 
Let's consider model in which each data point has its own mean and coefficients. This is called the saturated model. It basically replicates the data at hand. 

Using deviance, we can compare our fitted model to a saturated model. If the fitted model is behaves similiar to the saturated model, then the deviance can be well approximated by a chi-squared distribution with $m-n$ degrees of freedom. $m$ is number of the data points and $n$ is number of coefficients in our fitted model.

This satistical property of the deviance allows us perform a hypothesis test

$$H_0:\text{ the fitted model  is equivalent to the saturated model }$$
$$H_{\alpha}:\text{the fitted model is not equivalent to the saturated model}$$

`logit$deviance` is the deviance between saturated model and fitted model. `logit$df.residual` is equal to number of observations minus the number of coefficients in the fitted model. Using this, we can calculate the p value for the hypothesis test above.

```{r}
p_value = pchisq(logit$deviance, 
                 logit$df.residual, lower.tail = F)
print(p_value)
```

Since the p value is greater than 0.05, we fail to reject the null hypothesis. (This is a good thing.)

### Null Model

We can also use deviance to determine if our fitted model is better than the null model. The null model is  is a model with only a linear term. Like above, we can design a hypothesis test comparing the null model to the fitted model.



$$H_0 = \text{ the fitted model  is equivalent to the null model }$$
$$H_{\alpha} = \text{ the fitted model  is not equivalent to the null model } $$

In the limit of large data, it is known that the deviance follows a chi-squared distribution with parameter $n-1.$

`logit$deviance` is the deviance between saturated model and fitted model. `logit$df.residual` is equal to number of observations minus the number of coefficients in the fitted model. 

`logit$null.deviance` is the deviance between saturated model and the null model. `logit$df.null` is the number of observations minus 1.

Using this information, we can calculate the p value for the hypothesis test above.


```{r}
p_value = pchisq(logit$null.deviance-logit$deviance,
                 logit$df.null-logit$df.residual, lower.tail = F)
print(p_value)
```

Since the p value is less than one, we reject our null hypothesis. (This is a good thing.)

### Anova 

`anova` with argument `test="Chisq` allows us to compare change in deviance after sequencially adding terms our model.


```{r}
anova(logit,test="Chisq")
```


\newpage

# Poisson General Linear Model

A possion GLM is used to study *count* data (i.e. discrete numbers, $0,1,2,\cdots$). *Count* data describes the number of events that occur within a given time frame.

*insert plot of poisson distribution here*
```{r}


```

A possion GLM is most useful when studying data in which the mean and variable are approximately equal. If they are not not equal, the standard error of the model terms must adjusted to account for the assumption violation.

Poisson Regression has the following properties:

-  response count variables, $Y_i$, that follows a Possion distribution with mean $\mu_i$
- a systematic component, $\eta_i$, that relates the relates the explanatory variables,
$\eta_i = \sum_{j=1}^{n} \beta_j x_{ij}$$
- a link function, $log(\mu_i) = \sum_{j=1}^{n} \beta_j x_{ij}$


From Poisson regression, we learn the *mean* of each $Y_i$ given the associated the explanatory variables.

## Data

We will be consider the `bioChemists` data set in this section. This data set contains number of articles produced by PhD biochemistry student during the last 3 years of their PhD.

```{r}
attach(bioChemists)
summary(bioChemists)
```

 The data set also contains demographic data associated with each student.  data of the flower of certain plant species. The data set has five variables:

- *art* - number of articles produced by the student in the last 3 years of their PhD
- *fem* - gender
- *mar* - martial status
- *kid5* - number of children less than 5
- *phd* - pretige of PhD program
- *ment* - number of articles of the mentor in the last 3 years


```{r}
sapply(bioChemists, class)
```

I convert `bioChemists$kid5` from numeric to factor. This will be used later.

```{r}
bioChemists$kid5 <- factor(bioChemists$kid5, 
                            levels= unique(bioChemists$kid5),
                            labels= unique(bioChemists$kid5))
```


Plotting the bar graph of `bioChemists$art`, we can see than the data looks Poisson-like since there is large number of observations at 0.

```{r}
ggplot(bioChemists,aes(x=bioChemists$art))+ 
  geom_histogram(binwidth = 1, center = 1) +
  scale_x_continuous(breaks=seq(0,max(bioChemists$art), by = 1))+
  ylab("Frequency")+ xlab("data")+
  ggtitle("Histogram plot of the number of articles published by biochemist phd students in last 3 years")
```

We can "quantify" the Poission-ness by analyzing the mean and variance of the data.
```{r}
mean(bioChemists$art)
var(bioChemists$art)
```

Although mean and variance are not equal, we will still fit it to Poisson distribution. 


## Possion Regression with constant term

To model only the constant term, I use the formula `art ~ 1`. This formula is equivalent to

$$\log \mu_i  = \beta_0.$$

```{r}
poisson_model = glm(art ~ 1, family=poisson(link=log),data=bioChemists)
summary(poisson_model)
```

Note that the constant term is the log mean number of counts.

```{r}
print(coef(poisson_model))
```

```{r}
print(log(mean(bioChemists$art)))
```

## Goodness of fit

### Saturated model
We can again compare the current model to the saturated model (best possible fit).

```{r}
p_value = pchisq(poisson_model$deviance,
                 poisson_model$df.residual, lower.tail = F)
print(p_value)
```

Since our p value is less than 0.05, we reject the null hypothesis. The models are not equivalent.
 
### Null model

We can also compare the current model to the null model (worst possible fit).

```{r}
p_value = pchisq(poisson_model$null.deviance-poisson_model$deviance,
                poisson_model$df.null-poisson_model$df.residual, lower.tail = F)
print(p_value)
```

We fail to reject the null hypothesis. This makes sense since the models are literally the same thing.


## Possion Regression with martial status covariate

To model the martial status covariate, I use the formula `art ~ 1+mar`. This formula is equivalent to

$$\log \mu_i = \beta_0 + \beta_1 X_{1i} $$

where $$X_{1i} = \begin{cases}1 & \text{if mar = Married } \\ 0 & \text{otherwise} \end{cases}.$$

```{r}
poisson_model = glm(art~1+mar , family=poisson(link=log),data=bioChemists)
summary(poisson_model)
```

```{r}
plot(bioChemists$art,ylab='number of articles',xlab = 'Index')
points(poisson_model$fitted[bioChemists$mar=='Single'],col="red")
points(poisson_model$fitted[bioChemists$mar=='Married'],col="blue")
```

Graphically, we can see than that martial status is not good indicator of number articles published.

##Goodness of fit

### Saturated model

We can again compare the current model to the saturated model (best possible fit).

```{r}
p_value = pchisq(poisson_model$deviance,
                 poisson_model$df.residual, lower.tail = F)
print(p_value)
```

Since our p value is less than 0.05, we reject the null hypothesis. The models are not equivalent and our model is a bad fit.

### Null model

We can also compare the current model to the null model (worst possible fit).

```{r}
p_value = pchisq(poisson_model$null.deviance-logit$deviance,
                poisson_model$df.null-logit$df.residual, lower.tail = F)
print(p_value)
```

Since our p value is less than 0.05, we reject the null hypothesis. The models are not equivalent. Though our current model does not capture much deviance, the current model captures much more variance than the null model.


```{r}
anova(poisson_model,test="Chisq")
```



##  Possion Regression with martial status and children covariate

To model the martial status and children as covariates, I use the formula `art ~ 1+mar + kid5`. This formula is equivalent to

$$\log \mu_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \beta_4 X_{4i}$$

where 
$$X_{1i} = \begin{cases}1 & \text{if the ith data point is married} \\ 0 & \text{otherwise} \end{cases},$$
$$ X_{2i} = \begin{cases}1 & \text{if the number of children of ith data point is  1 } \\ 0 & \text{otherwise} \end{cases},$$
$$X_{3i} = \begin{cases}1 & \text{if the number of children of ith data point is 2 } \\ 0 & \text{otherwise} \end{cases}$$

$$\text{and } X_{4i} = \begin{cases}1 & \text{if the number of children of ith data point is 3 } \\ 0 & \text{otherwise} \end{cases}.$$
```{r}
poisson_model = glm(art ~ 1 + kid5 + mar,
                    family=poisson(link=log),data=bioChemists)
summary(poisson_model)
```
```{r}
plot(bioChemists$art,ylab='number of articles',xlab = 'Index')
single_bioChemists = bioChemists[bioChemists$mar=='Single',]

points(poisson_model$fitted[single_bioChemists$kid5== 0],col="blue",pch=1)
points(poisson_model$fitted[single_bioChemists$kid5== 1],col="yellow",pch=1)
points(poisson_model$fitted[single_bioChemists$kid5== 2],col="red",pch=1)
points(poisson_model$fitted[single_bioChemists$kid5== 3],col="green",pch=1)

```

```{r}
plot(bioChemists$art,ylab='number of articles',xlab = 'Index')
mar_bioChemists = bioChemists[bioChemists$mar=='Married',]

points(poisson_model$fitted[mar_bioChemists$kid5== 0],col="blue",pch=2)
points(poisson_model$fitted[mar_bioChemists$kid5== 1],col="yellow",pch=2)
points(poisson_model$fitted[mar_bioChemists$kid5== 2],col="red",pch=2)
points(poisson_model$fitted[mar_bioChemists$kid5== 3],col="green",pch=2)
```
Graphically, we can see than that martial status and number of children is not good indicator of number articles published.

## Goodness of Fit

### Saturated model 
We can again compare the current model to the saturated model (best possible fit).

```{r}
p_value = pchisq(poisson_model$deviance,
                 poisson_model$df.residual, lower.tail = F)
print(p_value)
```

Since our p value is less than 0.05, we reject the null hypothesis. The models are not equivalent.

### Null model

We can also compare the current model to the null model (worst possible fit). 
```{r}
p_value = pchisq(poisson_model$null.deviance-poisson_model$deviance,
                poisson_model$df.null-poisson_model$df.residual, lower.tail = F)
print(p_value)
```

### Anova

We can also determine the model terms that cause a significance reduction in deviance.

```{r}
anova(poisson_model,test="Chisq")
```


##  Possion Regression with continuous variables, mentor articles and martial status

To model the martial status and number of mentor articles as covariates, I use the formula `art ~ 1+mar + ment`. This formula is equivalent to

$$\log \mu_i = \beta_0 + \beta_1 X_{1i} + X_{2i}$$

where 
$$X_{1i} = \begin{cases}1 & \text{if the i data point is Married } \\ 0 & \text{otherwise} \end{cases}$$ 
and
$X_{2i}$  is the number of publications of the $i$th data point's mentor.

```{r}
poisson_model = glm(art ~ 1 + ment +mar,
                    family=poisson(link=log),data=bioChemists)
summary(poisson_model)
```

```{r}
plot(bioChemists$art,ylab='number of articles',xlab = 'Index')

points(poisson_model$fitted,col="blue",pch=1)

```
Graphically, we can see than that martial status and number of children is not good indicator of number articles published.

## Goodness of Fit

### Saturated model 
We can again compare the current model to the saturated model (best possible fit).

```{r}
p_value = pchisq(poisson_model$deviance,
                 poisson_model$df.residual, lower.tail = F)
print(p_value)
```

Since our p value is less than 0.05, we reject the null hypothesis. The models are not equivalent.

### Null model

We can also compare the current model to the null model (worst possible fit). 

```{r}
p_value = pchisq(poisson_model$null.deviance-poisson_model$deviance,
                poisson_model$df.null-poisson_model$df.residual, lower.tail = F)
print(p_value)
```

### Anova

We can also determine the model terms that cause a significance reduction in deviance.

```{r}
anova(poisson_model,test="Chisq")
```


\newpage

# Log-Linear Regression

Log-linear models allow us to model asscociation between between two or more variables in contingency table. In a log-linear model, there are no well defined explanatory/response variables. This is because we are focused more on the *interaction* between two variables.

## Contingency Table

Contingency table displays number of observations for a given combination of factors.

This definition is best represented by an example. 

```{r}
bioChemists$art_binary <- sapply(bioChemists$art,function(x) ifelse(x > 1, 1, 0))
bioChemists$ment_binary <- sapply(bioChemists$ment,function(x) ifelse(x > median(bioChemists$ment), 1, 0))

```

### One-Way Contingency Table

A one-way contingency table shows the counts according to one covariate.
```{r}
table(art_relative=bioChemists$art_binary)
```

This one-way contigency table shows that:

- there are 521 biochemists with 1 or less papers
- there are 394 biochemists with greater than 1 papers.

### Two-Way Contingency Table

A two-way contingency table shows the counts according to two covariates.

```{r}
table(art_relative=bioChemists$art_binary,ment=bioChemists$ment_binary)
```

This two-way contigency table shows that:

- there are 321 biochemists with 1 or less papers and with a mentor that produced less than or equal to 6 papers
- there are 200 biochemists with 1 or less papers and with a mentor that produced more than 6 papers
- there are 171 biochemists with more than 1 paper and with a mentor that produced less than or equal to 6 papers
- there are 200 biochemists with more than 1 paper and with a mentor that produced more than 6 papers

### Three-Way Contingency Table

A three-way contingency table shows the counts according to three covariates.

```{r}
table(art_relative=bioChemists$art_binary,ment=bioChemists$ment_binary,
      kid5=bioChemists$kid5)
```


This two-way contigency table shows that:

* With no children, 
    * there are 208 biochemists with 1 or less papers and with a mentor that produced less than or equal to 6 papers
    * there are 128 biochemists with 1 or less papers and with a mentor that produced more than 6 papers
    * there are 116 biochemists with more than 1 paper and with a mentor that produced less than or equal to 6 papers
    * there are 147 biochemists with more than 1 paper and with a mentor that produced more than 6 papers
    
* With 1 child, 
    * there are 66 biochemists with 1 or less papers and with a mentor that produced less than or equal to 6 papers
    * there are 46 biochemists with 1 or less papers and with a mentor that produced more than 6 papers
    * there are 38 biochemists with more than 1 paper and with a mentor that produced less than or equal to 6 papers
    * there are 45 biochemists with more than 1 paper and with a mentor that produced more than 6 papers
    
* With 2 children, 
    * there are 39 biochemists with 1 or less papers and with a mentor that produced less than or equal to 6 papers
    * there are 21 biochemists with 1 or less papers and with a mentor that produced more than 6 papers
    * there are 16 biochemists with more than 1 paper and with a mentor that produced less than or equal to 6 papers
    * there are 29 biochemists with more than 1 paper and with a mentor that produced more than 6 papers
    
* With 3 children, 
    * there are 8 biochemists with 1 or less papers and with a mentor that produced less than or equal to 6 papers
    * there are 5 biochemists with 1 or less papers and with a mentor that produced more than 6 papers
    * there are 1 biochemists with more than 1 paper and with a mentor that produced less than or equal to 6 papers
    * there are 2 biochemists with more than 1 paper and with a mentor that produced more than 6 papers


## Log-linear regression
For a two way contingency table, log-linear GLMs have the following properties:

- count response variables, $Y_{ij}$, which is the number of entries in the (i,j)th cell of the table. $Y_{ij}$ follows a Possion distribution with mean $\mu_{ij}$.
- a systematic component, $\eta_i$, that relates the relates the explanatory variables,
$$\eta_{ij} = \sum_{j=1}^{n} \beta_{k} X_{ijk}$$
- a link function that relates the mean of the random to the systematic component
$$\log \mu_{ij} = \sum_{k=1}^{n}\beta_{k} X_{ijk}$$


## Independent Model for two-way contigency table

We use log-linear model to model the group mean count of each cell of the contingency table. Remember, using a log-linear model, our primary goal is to learn the interaction effects between covariates.  

Again, we build the same two-way contingency table. We need to convert the contigency table in a form that is acceptable to `glm`. We do this below.

```{r}
contigency_table = table(art_relative=bioChemists$art_binary,ment=bioChemists$ment_binary)
contigency_table.df = as.data.frame(contigency_table)
print(contigency_table.df)
```

Assuming each number of articles and mentor do not affect each other, we build a model of the cell count that does not take into account interaction effects. Such a model is called the *independent* model. To do this, we use formula `Freq ~ art_relative + ment`. 


```{r}
log_linear_model_int <- glm(Freq ~ art_relative + ment, 
            data = contigency_table.df, family = poisson)
summary(log_linear_model_int)
```

## Goodness of fit

We compare the current model to the saturated model (best possible fit).

```{r}
p_value = pchisq(log_linear_model_int$deviance,
                 log_linear_model_int$df.residual, lower.tail = F)
print(p_value)
```

Since our p value is less than 0.05, we reject the null hypothesis. The models are not equivalent.

## Saturated Model for the two-way contingency table 

Assuming each number of articles and mentor affect each other, we build a model of the cell count that takes into account all interaction effects. Such a model is called the *saturated* model. To do this, we use formula `Freq ~ art_relative*ment`. 

```{r}
log_linear_model_sat <- glm(Freq ~ art_relative*ment, 
            data = contigency_table.df, family = poisson)
summary(log_linear_model_sat)
```

## Goodness of fit

We compare the current model to the saturated model (best possible fit).


```{r}
p_value = pchisq(0,
                 log_linear_model_sat$df.residual, lower.tail = F)
print(p_value)
```

We fail to reject the null hypothesis. This makes sense since the models are literally the same thing.

## Model Comparison

We use `anova` with `test='Chisq'` to compare the independent and saturated model. 

```{r}
anova(log_linear_model_int,log_linear_model_sat,test='Chisq')
```

From `anova`, we can see that the saturated model provides a statisically significant result.

We use also `anova` to determine what caused the significant decrease in the deviance.

```{r}
anova(log_linear_model_sat,test='Chisq')
```
Adding `art_relative:ment` to the independent model caused significant decrease in deviance.


## Independent Model for the three-way contingency table 

Again, we build the same three-way contingency table. We need to convert the contigency table in a form that is acceptable to `glm`. We do this below.

```{r}
contigency_table = table(art_relative=bioChemists$art_binary,
                         ment=bioChemists$ment_binary,
                         kid5=bioChemists$kid5)
contigency_table.df = as.data.frame(contigency_table)
```

To create the *independent* model for the three-way contingency table, we use formula `Freq ~  art_relative + ment + kid5`. 

```{r}
log_linear_model_int <- glm(Freq ~ art_relative + ment + kid5, 
            data = contigency_table.df, family = poisson)
summary(log_linear_model_int)
```

## Goodness of fit

We compare the current model to the saturated model (best possible fit).

```{r}
p_value = pchisq(log_linear_model_int$deviance,
                 log_linear_model_int$df.residual, lower.tail = F)
print(p_value)
```

Since our p value is less than 0.05, we reject the null hypothesis. The models are not equivalent.


## Saturated Model

To create the *independent* model for the three-way contingency table, we use formula `Freq ~  art_relative*ment*kid5`. 


```{r}
log_linear_model_sat <- glm(Freq ~ art_relative*ment*kid5, 
            data = contigency_table.df, family = poisson)
summary(log_linear_model_sat)
```
## Goodness of fit

We compare the current model to the saturated model (best possible fit).

```{r}
p_value = pchisq(log_linear_model_sat$deviance,
                 log_linear_model_sat$df.residual, lower.tail = F)
print(p_value)
```
We fail to reject the null hypothesis. This makes sense since the models are literally the same thing.



##  Model Comparison

We use `anova` with `test='Chisq'` to compare the independent and saturated model. 

```{r}
anova(log_linear_model_int,log_linear_model_sat,test='Chisq')
```

From `anova`, we can see that the saturated model provides a statisically significant result.

We use also `anova` to determine what caused the significant decrease in the deviance.

```{r}
anova(log_linear_model_sat,test='Chisq')
```
Adding `art_relative:ment` to the independent model caused significant decrease in deviance.

\newpage 

# Hierarchical modeling

```{r}
student_data <- read.csv("data/hsb1.csv")
school_data  <- read.csv("data/hsb2.csv")
student_data$ses_grandmean <- student_data$ses - mean(student_data$ses) # Grand-mean centered student SES 
school_data$sm_ses_grandmean <- school_data$meanses - mean(school_data$meanses) # Grand-mean centered school SES

data <- merge(student_data, school_data, by = "id")

ses_group_mean <- aggregate(data$ses, list(data$id), FUN = mean, data = data) # Group-mean centered student SES
names(ses_group_mean)<- c('id','groupmeanSES')
data <- merge(data, ses_group_mean, by = "id")

groups <- unique(data$id)[sample(1:160,20)]
subset <- data[data$id%in%groups, ]

xyplot(mathach ~ ses | as.factor(id), subset,
       col.line = 'black',
       type = c("p", "r"),
       main = 'Variability in Math Achievement ~ SES Relationship')

xyplot(mathach ~ ses |as.factor(id), subset,
                        col.line = 'black',
                        type = c("p", "smooth"),
                        main = 'Variability in Math Achievement ~ SES Relationship')

xyplot(mathach ~ ses, subset,
       type = c("p", "smooth"),
       group = data$id,
       main = 'Variability in Math Achievement ~ SES Relationship')

unconditional <- lmer(mathach ~ 1 + (1|id), data = data)
summary(unconditional) # on p-values in nlme: https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html
confint(unconditional) # you can also just calculate an approximate 95% confidence interval yourself: estimate +/- 2(SE) (in this case: 12.64 +/- 2(0.24))
unconditional_2 <- lme(mathach ~ 1, random = ~ 1 | id, data = data)
summary(unconditional_2)

random_intercept_fixed_slope <- lmer(mathach ~ 1 + groupmeanSES + (1|id), data = data)
summary(random_intercept_fixed_slope)
confint(random_intercept_fixed_slope)
random_intercept_fixed_slope_2 <- lme(mathach ~ 1 + groupmeanSES, random = ~ 1 | id, data = data)
summary(random_intercept_fixed_slope_2)

random_intercept_random_slope <- lmer(mathach ~ 1 + groupmeanSES + (1 + groupmeanSES|id), data = data)
summary(random_intercept_random_slope)
random_intercept_random_slope_2 <- lme(mathach ~ 1 + groupmeanSES, random = ~ 1 + groupmeanSES | id, data = data)
summary(random_intercept_random_slope_2)

fixed_intercept_random_slope <- lmer(mathach ~ 1 + groupmeanSES + (0 + groupmeanSES|id), data = data)
summary(fixed_intercept_random_slope)
fixed_intercept_random_slope_2 <- lme(mathach ~ 1 + groupmeanSES, random = ~ 0 + groupmeanSES | id, data = data)
summary(fixed_intercept_random_slope_2)

fixed_slope_level_two_variable <- lmer(mathach ~ 1 + groupmeanSES + sm_ses_grandmean + (1|id), data = data)
summary(fixed_slope_level_two_variable)
fixed_slope_level_two_variable_2 <- lme(mathach ~ 1 + groupmeanSES + sm_ses_grandmean, random = ~ 1 | id, data = data)
summary(fixed_slope_level_two_variable_2)

random_slope_level_two_variable <- lmer(mathach ~ 1 + groupmeanSES + sm_ses_grandmean + (1 + groupmeanSES|id), data = data)
summary(random_slope_level_two_variable)
random_slope_level_two_variable_2 <- lme(mathach ~ 1 + groupmeanSES + sm_ses_grandmean, random = ~ 1 + groupmeanSES | id, data = data)
summary(random_slope_level_two_variable_2)  

fixed_slope_cl_interaction <- lmer(mathach ~ 1 + groupmeanSES*sm_ses_grandmean + (1|id), data = data)
summary(fixed_slope_cl_interaction)
fixed_slope_cl_interaction_2 <- lme(mathach ~ 1 + groupmeanSES*sm_ses_grandmean, random = ~ 1 | id, data = data)
summary(fixed_slope_cl_interaction_2)

random_slope_cl_interaction <- lmer(mathach ~ 1 + groupmeanSES*sm_ses_grandmean + (1 + groupmeanSES|id), data = data)
summary(random_slope_cl_interaction)
random_slope_cl_interaction_2 <- lme(mathach ~ 1 + groupmeanSES*sm_ses_grandmean, random = ~ 1 + groupmeanSES | id, data = data)
summary(random_slope_cl_interaction_2)

logit_random_intercept_and_slope <- glmer(minority ~ groupmeanSES + (1 + groupmeanSES | id), data = data, 
                                    family = binomial(link="logit"))
summary(logit_random_intercept_and_slope)

specified_variance_covariance_matrix_for_random_effects <- lme(mathach ~ 1 + groupmeanSES*sm_ses_grandmean, random = ~ 1 + groupmeanSES | id, 
                                                               correlation = corAR1(), data = data) # just an example, not needed in this case (useful for growth curve models)!
summary(specified_variance_covariance_matrix_for_random_effects)
```