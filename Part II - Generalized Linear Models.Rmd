---
title: "Part II: Generalized Linear Models"
output:
  html_document: default
  pdf_document: default
---

# Load Packages 

Again, we must load the packages that will be used in the first part of this workshop.


```{r, message=FALSE, warning=FALSE }
library(pastecs, quietly = TRUE)
library(lm.beta,  quietly = TRUE)
library(lmtest,  quietly = TRUE)
library(foreign,  quietly = TRUE)
library(lattice,  quietly = TRUE)
library(lme4,  quietly = TRUE)
library(nlme,  quietly = TRUE)
library(survival,  quietly = TRUE)
library(dplyr,  quietly = TRUE)
library(ggfortify,  quietly = TRUE)
library(survminer,  quietly = TRUE)
library(rms,  quietly = TRUE)
library(MASS, quietly = TRUE)
```

# Generalized linear models

A generalized linear model (GLM) has three:

- a random component with mean $\mu$. Generally, the random component is the response variable $Y_i.$
- a systematic component, $\eta_i$, that relates the relates the explanatory variables,
$$\eta_i = \sum_{j=i}^{n} \beta_j x_{ij}$$
- a link function that relates the mean of the random to the systematic component
$$g(\mu) = \eta_i$$

# Logistic regression

Logistic regression is a GLM used the model binary (0 or 1) data. The response variable must be binary and is assumed to follow a bernoulli distribution. 

That said, logistic regression has the following properties:
-  a response binary variable, $Y_i$, that follows a bernoulli distribution with mean $\pi_i$.
- a systematic component, $\eta_i$, that relates the relates the explanatory variables,
$$\eta_i = \sum_{j=1}^{n} \beta_j x_{ij}$$
- a link function that relates the mean of the random to the systematic component
$$\log\left(\frac{\pi_i}{1-\pi_i}\right) = \sum_{j=i}^{n} \beta_j x_{ij}.$$ $\log\left(\frac{\pi_i}{1-\pi_i}\right)$ is known as the log odds.

## Data 

Using the iris data, we create binary data. We add the column `Sepal.Width_binary` to iris. If the `Sepal.Width` is greater than the median then the associated value in `Sepal.Width_binary` is 1. Otherwise, `Sepal.Width_binary` is 0.

```{r}
data <- iris
data$Sepal.Width_binary <- ifelse(data$Sepal.Width >= median(data$Sepal.Width), 1, 0)
```

## Logistic Regression with only the constant term

Fitting only a constant term, the systematic component is
$$\eta_i = \beta_0$$
```{r}
logit <- glm(Sepal.Width_binary ~ 1, data = data, family = "binomial")
summary(logit)
```

```{r}
p_avg <- mean(data$Sepal.Width_binary)
log_odds_avg <- log(p_avg/(1-p_avg))
print(log_odds_avg)
```

## Logistic Regression with Species  

Fitting the species term, the systematic component is
$$\eta_i = 1 + \beta_2 X_{1i} + \beta_3 X_{2i}.$$

where 
$$  X_{1i} = \begin{cases} 1 & \text{if } i\text{th data point is versicolor}\\ 0 &  \text{otherwise}\end{cases}, \, X_{2i} = \begin{cases} 1 & \text{if } i \text{th data point is virginica}\\ 0 & \text{otherwise}\end{cases}$$


```{r}
logit <- glm(Sepal.Width_binary ~ as.factor(Species), data = data, family = "binomial")
summary(logit)
```

Let's compare the results to the average log odds of each Species group
```{r}
log_odds_avg_fun <- function(data){
  p_avg <- mean(data)
  log_odds_avg <- log(p_avg/(1-p_avg))
  return(log_odds_avg)
}

tapply(data$Sepal.Width_binary,
       data$Species, log_odds_avg_fun)
```

The intercept corresponds to the average log odds of setosa as we would expect. However, the other coefficients do not correspond to the average log odds of the other species. Why?

From the formula, $\eta_i = 1 + \beta_2 X_{2i} + \beta_3 X_{3i}$, the log odds of versicolor actually corresponds to $1+\beta_2$. The log odds of versicolor actually corresponds to $1+\beta_3$.

```{r}
coefficients<-unname(coef(logit))
print(c(coefficients[1],coefficients[1]+coefficients[2],
        coefficients[1]+coefficients[3]))
```

## Logistic Regression with Sepal.Length

Fitting the species term, the systematic component is

$$\eta_i =  \beta_3 X_{1i}.$$

where 
$$  X_{1i} = \begin{cases} 1 & \text{if } i\text{th data point is versicolor}\\ 0 &  \text{otherwise}\end{cases}, \, X_{2i} = \begin{cases} 1 & \text{if } i \text{th data point is virginica}\\ 0 & \text{otherwise}\end{cases}$$ and $X_{3i} = \text{Sepal.Length of the }i\text{th  data point}.$

```{r}
logit <- glm(Sepal.Width_binary ~ Sepal.Length, 
             data = data, family = "binomial")
summary(logit)
```


```{r}
plot(Sepal.Width_binary~Sepal.Length, data=data)
points(data$Sepal.Length[order(data$Sepal.Length)],
       logit$fitted[order(data$Sepal.Length)],  col="red")
title(main="Data with Fitted Logistic Regression Line")
```

## Logistic Regression with Species and Sepal.Length

Fitting the species term, the systematic component is
$$\eta_i = 1 + \beta_2 X_{1i} + \beta_3 X_{2i} + \beta_3 X_{3i}.$$

where 
$$  X_{1i} = \begin{cases} 1 & \text{if } i\text{th data point is versicolor}\\ 0 &  \text{otherwise}\end{cases}, \, X_{2i} = \begin{cases} 1 & \text{if } i \text{th data point is virginica}\\ 0 & \text{otherwise}\end{cases}$$ and $X_{3i} = \text{Sepal.Length of the }i\text{th  data point}.$

Fitting the logistic model accordingly,

```{r}
logit <- glm(Sepal.Width_binary ~ Species +Sepal.Length, 
             data = data, family = "binomial")
summary(logit)
```
Plot the results for each species, we get that
```{r}
plot(data[data$Species == "setosa", ]$Sepal.Length, 
     data[data$Species == "setosa", ]$Sepal.Width_binary, 
     xlim=as.matrix(range(data$Sepal.Length)),
     xlab = 'Sepal Length',  ylab= 'Sepal Width binary',
     main= 'Scatter plot of sepal length vs sepal width')

points(data$Sepal.Length[data$Species == "setosa"],
       logit$fitted[data$Species == "setosa"],  pch=15,
       col="red")
```

```{r}
plot(data[data$Species == "versicolor", ]$Sepal.Length,
     data[data$Species == "versicolor", ]$Sepal.Width_binary,
     xlim=as.matrix(range(data$Sepal.Length)),
     xlab = 'Sepal Length',  ylab= 'Sepal Width binary',
     main= 'Scatter plot of sepal length vs sepal width')

points(data$Sepal.Length[data$Species == "versicolor"],
       logit$fitted[data$Species == "versicolor"],  pch=15,
       col="yellow")
```

```{r}
plot(data[data$Species == "virginica", ]$Sepal.Length,
     data[data$Species == "virginica", ]$Sepal.Width_binary,
     xlim=as.matrix(range(data$Sepal.Length)),
     xlab = 'Sepal Length',  ylab= 'Sepal Width binary',
     main= 'Scatter plot of sepal length vs sepal width')

points(data$Sepal.Length[data$Species == "virginica"],
       logit$fitted[data$Species == "virginica"],  pch=15,
       col="blue")
```


## Deviance

 For general linear models, we use *deviance* to the "distance" between two models. Deviance is the difference in log likelihood of the models multipled by 2.
 
### Saturated Model
 
Let's consider model in which each data point has its own mean and coefficients. This is called the saturated model. It basically replicates the data at hand. 

Using deviance, we can compare our fitted model to a saturated model. If the fitted model is behaves similiar to the saturated model, then the deviance can be well approximated by a chi-squared distribution with $m-n$ degrees of freedom. $m$ is number of the data points and $n$ is number of coefficients in our fitted model.

This satistical property of the deviance allows us perform a hypothesis test

$$H_0:\text{ the fitted model  is equivalent to the saturated model }$$
$$H_{\alpha}:\text{the fitted model is not equivalent to the saturated model}$$

`logit$deviance` is the deviance between saturated model and fitted model. `logit$df.residual` is equal to number of observations minus the number of coefficients in the fitted model. Using this, we can calculate the p value for the hypothesis test above.

```{r}
p_value = 1 - pchisq(logit$deviance, logit$df.residual)
print(p_value)
```

Since the p value is less than 0.05, we fail to reject the null hypothesis. (This is a good thing.)

### Null Model

We can also use deviance to determine if our fitted model is better than the null model. The null model is  is a model with only a linear term. Like above, we can design a hypothesis test comparing the null model to the fitted model.



$$H_0 = \text{ the fitted model  is equivalent to the null model }$$
$$H_{\alpha} = \text{ the fitted model  is not equivalent to the null model } $$

In the limit of large data, it is known that the deviance follows a chi-squared distribution with parameter $n-1.$

`logit$deviance` is the deviance between saturated model and fitted model. `logit$df.residual` is equal to number of observations minus the number of coefficients in the fitted model. 

`logit$null.deviance` is the deviance between saturated model and the null model. `logit$df.null` is the number of observations minus 1.

Using this information, we can calculate the p value for the hypothesis test above.


```{r}
p_value = 1 - pchisq(logit$null.deviance-logit$deviance,
                     logit$df.null-logit$df.residual)
print(p_value)
```

Since the p value is less than one, we reject our null hypothesis. (This is a good thing.)

### Anova 

Sequencial comparison of model terms by deviance

```{r}
anova(logit,test="Chisq")
```


\newpage

# Poisson and Quasi-Poisson Regression

```{r}
attach(grouseticks)
summary(grouseticks)
```

```{r}
sapply(grouseticks, class)


ggplot(grouseticks,aes(x=grouseticks$TICKS))+ 
  geom_histogram(binwidth = 1, center = 0.5) +
  scale_x_continuous(breaks=seq(1,max(grouseticks$TICKS), by = 5))+
  ylab("Count")+ xlab("data")+
  ggtitle("Histogram plot of the number of ticks on the heads of red grouse chicks")
```

```{r}
model = glm(TICKS~ 1, family=poisson(link=log),data=grouseticks)
summary(model)
```


```{r}
print(coef(model))
```

```{r}
head(data.frame(grouseticks$TICKS,model$fitted))
```

```{r}
head(model$linear.predictors)
head(exp(model$linear.predictors))
```

### Hypothesis test for goodness of fit
```{r}
print(1-pchisq(model$deviance,model$df.residual))
```

plot data comparison
```{r}
df_original = data.frame(data=grouseticks$TICKS)
df_fitted = data.frame(data=model$fitted)
plot(df_original$data,df_fitted$data,ylab='fit data',xlab='original data')
```


add a Covariate to the fit -- treatment
```{r}
model = glm(TICKS~ 1 +HEIGHT + LOCATION, family=poisson(link=log),grouseticks)
summary(model)
print(1-pchisq(model$deviance,model$df.residual))
```

```{r}
df_fitted = data.frame(data=model$fitted)
df_fitted$name = "fitted"
plot(df_original$data,df_fitted$data)
```

### add a Covariate to the fit -- treatment, age

```{r}
#model = glm(y ~ 1 +trt*age, family=poisson(link=log),df)
#summary(model)
```

```{r}
print(1-pchisq(model$deviance,model$df.residual))
df_fitted = data.frame(data=model$fitted)
combined=rbind(df_original,df_fitted)
plot(df_original$data,df_fitted$data)
```

## Overdispersion might be present -- try Quasipossion  
```{r}
#model = glm(y ~ 1 +trt*age, family=quasipoisson(link=log),df)
#summary(model)

#print(1-pchisq(model$deviance,model$df.residual))

#df_fitted = data.frame(data=model$fitted)

#combined=rbind(df_original,df_fitted)

#plot(df_original$data,df_fitted$data)
#detach(epil)
```