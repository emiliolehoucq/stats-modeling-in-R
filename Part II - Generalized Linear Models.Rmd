---
title: "Part II: Generalized Linear Models"
output:
  pdf_document: default
  html_document: default
---

# Load Packages 

Again, we must load the packages that will be used in the first part of this workshop.


```{r, message=FALSE, warning=FALSE }
library(pastecs, quietly = TRUE)
library(lm.beta,  quietly = TRUE)
library(lmtest,  quietly = TRUE)
library(foreign,  quietly = TRUE)
library(lattice,  quietly = TRUE)
library(lme4,  quietly = TRUE)
library(nlme,  quietly = TRUE)
library(survival,  quietly = TRUE)
library(dplyr,  quietly = TRUE)
library(ggfortify,  quietly = TRUE)
library(survminer,  quietly = TRUE)
library(rms,  quietly = TRUE)
library(MASS, quietly = TRUE)
library(pscl, quietly = TRUE)

```

# Generalized linear models

A generalized linear model (GLM) has three:

- a random component with mean $\mu$. Generally, the random component is the response variable $Y_i.$
- a systematic component, $\eta_i$, that relates the relates the explanatory variables,
$$\eta_i = \sum_{j=i}^{n} \beta_j x_{ij}$$
- a link function that relates the mean of the random to the systematic component
$$g(\mu) = \eta_i$$

# Logistic regression

Logistic regression is a GLM used the model binary (0 or 1) data. The response variable must be binary and is assumed to follow a bernoulli distribution. 

That said, logistic regression has the following properties:
-  a response binary variable, $Y_i$, that follows a bernoulli distribution with mean $\pi_i$.
- a systematic component, $\eta_i$, that relates the relates the explanatory variables,
$$\eta_i = \sum_{j=1}^{n} \beta_j x_{ij}$$
- a link function that relates the mean of the random to the systematic component
$$\log\left(\frac{\pi_i}{1-\pi_i}\right) = \sum_{j=i}^{n} \beta_j x_{ij}.$$ $\log\left(\frac{\pi_i}{1-\pi_i}\right)$ is known as the log odds.

## Data 

Using the iris data, we create binary data. We add the column `Sepal.Width_binary` to iris. If the `Sepal.Width` is greater than the median then the associated value in `Sepal.Width_binary` is 1. Otherwise, `Sepal.Width_binary` is 0.

```{r}
data <- iris
data$Sepal.Width_binary <- ifelse(data$Sepal.Width >= median(data$Sepal.Width), 1, 0)
```

## Logistic Regression with only the constant term

Fitting only a constant term, the systematic component is
$$\eta_i = \beta_0$$
```{r}
logit <- glm(Sepal.Width_binary ~ 1, data = data, family = "binomial")
summary(logit)
```

```{r}
p_avg <- mean(data$Sepal.Width_binary)
log_odds_avg <- log(p_avg/(1-p_avg))
print(log_odds_avg)
```

## Logistic Regression with Species  

Fitting the species term, the systematic component is
$$\eta_i = 1 + \beta_2 X_{1i} + \beta_3 X_{2i}.$$

where 
$$  X_{1i} = \begin{cases} 1 & \text{if } i\text{th data point is versicolor}\\ 0 &  \text{otherwise}\end{cases}, \, X_{2i} = \begin{cases} 1 & \text{if } i \text{th data point is virginica}\\ 0 & \text{otherwise}\end{cases}$$


```{r}
logit <- glm(Sepal.Width_binary ~ as.factor(Species), data = data, family = "binomial")
summary(logit)
```

Let's compare the results to the average log odds of each Species group
```{r}
log_odds_avg_fun <- function(data){
  p_avg <- mean(data)
  log_odds_avg <- log(p_avg/(1-p_avg))
  return(log_odds_avg)
}

tapply(data$Sepal.Width_binary,
       data$Species, log_odds_avg_fun)
```

The intercept corresponds to the average log odds of setosa as we would expect. However, the other coefficients do not correspond to the average log odds of the other species. Why?

From the formula, $\eta_i = 1 + \beta_2 X_{2i} + \beta_3 X_{3i}$, the log odds of versicolor actually corresponds to $1+\beta_2$. The log odds of versicolor actually corresponds to $1+\beta_3$.

```{r}
coefficients<-unname(coef(logit))
print(c(coefficients[1],coefficients[1]+coefficients[2],
        coefficients[1]+coefficients[3]))
```

## Logistic Regression with Sepal.Length

Fitting the species term, the systematic component is

$$\eta_i =  \beta_3 X_{1i}.$$

where 
$$  X_{1i} = \begin{cases} 1 & \text{if } i\text{th data point is versicolor}\\ 0 &  \text{otherwise}\end{cases}, \, X_{2i} = \begin{cases} 1 & \text{if } i \text{th data point is virginica}\\ 0 & \text{otherwise}\end{cases}$$ and $X_{3i} = \text{Sepal.Length of the }i\text{th  data point}.$

```{r}
logit <- glm(Sepal.Width_binary ~ Sepal.Length, 
             data = data, family = "binomial")
summary(logit)
```


```{r}
plot(Sepal.Width_binary~Sepal.Length, data=data)
points(data$Sepal.Length[order(data$Sepal.Length)],
       logit$fitted[order(data$Sepal.Length)],  col="red")
title(main="Data with Fitted Logistic Regression Line")
```

## Logistic Regression with Species and Sepal.Length

Fitting the species term, the systematic component is
$$\eta_i = 1 + \beta_2 X_{1i} + \beta_3 X_{2i} + \beta_3 X_{3i}.$$

where 
$$  X_{1i} = \begin{cases} 1 & \text{if } i\text{th data point is versicolor}\\ 0 &  \text{otherwise}\end{cases}, \, X_{2i} = \begin{cases} 1 & \text{if } i \text{th data point is virginica}\\ 0 & \text{otherwise}\end{cases}$$ and $X_{3i} = \text{Sepal.Length of the }i\text{th  data point}.$

Fitting the logistic model accordingly,

```{r}
logit <- glm(Sepal.Width_binary ~ Species +Sepal.Length, 
             data = data, family = "binomial")
summary(logit)
```
Plot the results for each species, we get that
```{r}
plot(data[data$Species == "setosa", ]$Sepal.Length, 
     data[data$Species == "setosa", ]$Sepal.Width_binary, 
     xlim=as.matrix(range(data$Sepal.Length)),
     xlab = 'Sepal Length',  ylab= 'Sepal Width binary',
     main= 'Scatter plot of sepal length vs sepal width')

points(data$Sepal.Length[data$Species == "setosa"],
       logit$fitted[data$Species == "setosa"],  pch=15,
       col="red")
```

```{r}
plot(data[data$Species == "versicolor", ]$Sepal.Length,
     data[data$Species == "versicolor", ]$Sepal.Width_binary,
     xlim=as.matrix(range(data$Sepal.Length)),
     xlab = 'Sepal Length',  ylab= 'Sepal Width binary',
     main= 'Scatter plot of sepal length vs sepal width')

points(data$Sepal.Length[data$Species == "versicolor"],
       logit$fitted[data$Species == "versicolor"],  pch=15,
       col="yellow")
```

```{r}
plot(data[data$Species == "virginica", ]$Sepal.Length,
     data[data$Species == "virginica", ]$Sepal.Width_binary,
     xlim=as.matrix(range(data$Sepal.Length)),
     xlab = 'Sepal Length',  ylab= 'Sepal Width binary',
     main= 'Scatter plot of sepal length vs sepal width')

points(data$Sepal.Length[data$Species == "virginica"],
       logit$fitted[data$Species == "virginica"],  pch=15,
       col="blue")
```


## Deviance

 For general linear models, we use *deviance* to the compare to two different models. Deviance is the difference in log likelihood of the models multipled by 2.
 
### Saturated Model
 
Let's consider model in which each data point has its own mean and coefficients. This is called the saturated model. It basically replicates the data at hand. 

Using deviance, we can compare our fitted model to a saturated model. If the fitted model is behaves similiar to the saturated model, then the deviance can be well approximated by a chi-squared distribution with $m-n$ degrees of freedom. $m$ is number of the data points and $n$ is number of coefficients in our fitted model.

This satistical property of the deviance allows us perform a hypothesis test

$$H_0:\text{ the fitted model  is equivalent to the saturated model }$$
$$H_{\alpha}:\text{the fitted model is not equivalent to the saturated model}$$

`logit$deviance` is the deviance between saturated model and fitted model. `logit$df.residual` is equal to number of observations minus the number of coefficients in the fitted model. Using this, we can calculate the p value for the hypothesis test above.

```{r}
p_value = pchisq(logit$deviance, 
                 logit$df.residual, lower.tail = F)
print(p_value)
```

Since the p value is less than 0.05, we fail to reject the null hypothesis. (This is a good thing.)

### Null Model

We can also use deviance to determine if our fitted model is better than the null model. The null model is  is a model with only a linear term. Like above, we can design a hypothesis test comparing the null model to the fitted model.



$$H_0 = \text{ the fitted model  is equivalent to the null model }$$
$$H_{\alpha} = \text{ the fitted model  is not equivalent to the null model } $$

In the limit of large data, it is known that the deviance follows a chi-squared distribution with parameter $n-1.$

`logit$deviance` is the deviance between saturated model and fitted model. `logit$df.residual` is equal to number of observations minus the number of coefficients in the fitted model. 

`logit$null.deviance` is the deviance between saturated model and the null model. `logit$df.null` is the number of observations minus 1.

Using this information, we can calculate the p value for the hypothesis test above.


```{r}
p_value = pchisq(logit$null.deviance-logit$deviance,
                 logit$df.null-logit$df.residual, lower.tail = F)
print(p_value)
```

Since the p value is less than one, we reject our null hypothesis. (This is a good thing.)

### Anova 

Sequencial comparison of model terms by deviance

```{r}
anova(logit,test="Chisq")
```


\newpage

# Poisson General Linear Model

A possion GLM is used to study *count* data (i.e. discrete numbers, $0,1,2,\cdots$). *Count* data describes the number of events that occur within a given time frame.

*insert plot of poisson distribution here*
```{r}


```

A possion GLM is most useful when studying data in which the mean and variable are approximately equal. If they are not not equal, the standard error of the model terms must adjusted to account for the assumption violation.

Poisson Regression has the following properties:
-  a response binary variable, $Y_i$, that follows a Possion distribution with mean $\mu_i$
- a systematic component, $\eta_i$, that relates the relates the explanatory variables,
$\eta_i = \sum_{j=1}^{n} \beta_j x_{ij}$$
- a link function, $log(\mu_i) = \sum_{j=1}^{n} \beta_j x_{ij}$


From Poisson regression, we learn the *mean* of each $Y_i$ given the associated the explanatory variables.

## Data

We will be consider the `bioChemists` data set in this section. This data set contains number of articles produced by PhD biochemistry student during the last 3 years of their PhD.

```{r}
attach(bioChemists)
summary(bioChemists)
```

 The data set also contains demographic data associated with each student.  data of the flower of certain plant species. The data set has five variables:

- *art* - number of articles produced by the student in the last 3 years of their PhD
- *fem* - gender
- *mar* - martial status
- *kid5* - number of children less than 5
- *phd* - pretige of PhD program
- *ment* - number of articles of the mentor in the last 3 years


```{r}
sapply(bioChemists, class)
```

I convert `bioChemists$kid5` from numeric to factor. This will be used later.

```{r}
bioChemists$kid5 <- factor(bioChemists$kid5, 
                            levels= unique(bioChemists$kid5),
                            labels= unique(bioChemists$kid5))
```


Plotting the bar graph of `bioChemists$art`, we can see than the data looks Poisson-like since there is large number of observations at 0.

```{r}
ggplot(bioChemists,aes(x=bioChemists$art))+ 
  geom_histogram(binwidth = 1, center = 1) +
  scale_x_continuous(breaks=seq(0,max(bioChemists$art), by = 1))+
  ylab("Frequency")+ xlab("data")+
  ggtitle("Histogram plot of the number of articles published by biochemist phd students in last 3 years")
```

We can "quantify" the Poission-ness by analyzing the mean and variance of the data.
```{r}
mean(bioChemists$art)
var(bioChemists$art)
```

Although mean and variance are not equal, we will still fit it to Poisson distribution. 


## Possion Regression with constant term

```{r}
poisson_model = glm(art ~ 1, family=poisson(link=log),data=bioChemists)
summary(poisson_model)
```

The constant term is the mean number of counts.

```{r}
print(coef(poisson_model))
```

```{r}
print(log(mean(bioChemists$art)))
```

### Hypothesis test for goodness of fit

**TODO**should i reject the null in this case?

```{r}
p_value = pchisq(poisson_model$deviance,
                 poisson_model$df.residual, lower.tail = F)
print(p_value)
```


```{r}
p_value = pchisq(poisson_model$null.deviance-logit$deviance,
                poisson_model$df.null-logit$df.residual, lower.tail = F)
print(p_value)
```


## Possion Regression with martial status covariate

```{r}
poisson_model = glm(art~ 1 + mar , family=poisson(link=log),data=bioChemists)
summary(poisson_model)
```

```{r}
plot(bioChemists$art)
points(poisson_model$fitted[bioChemists$mar=='Single'],col="red")
points(poisson_model$fitted[bioChemists$mar=='Married'],col="blue")
```

### Hypothesis test for goodness of fit

```{r}
p_value = pchisq(poisson_model$deviance,
                 poisson_model$df.residual, lower.tail = F)
print(p_value)
```


```{r}
p_value = pchisq(poisson_model$null.deviance-logit$deviance,
                poisson_model$df.null-logit$df.residual, lower.tail = F)
print(p_value)
```

```{r}
anova(poisson_model,test="Chisq")
```



##  Possion Regression with martial status and children covariate

*TODO* why are interaction factors NA

```{r}
poisson_model = glm(art ~ 1 + kid5 + mar,
                    family=poisson(link=log),data=bioChemists)
summary(poisson_model)
```

### Hypothesis test for goodness of fit

```{r}
p_value = pchisq(poisson_model$deviance,
                 poisson_model$df.residual, lower.tail = F)
print(p_value)
```


```{r}
p_value = pchisq(poisson_model$null.deviance-logit$deviance,
                poisson_model$df.null-logit$df.residual, lower.tail = F)
print(p_value)
```

```{r}
anova(poisson_model,test="Chisq")
```


##  Possion Regression with martial status, children and number of mentor articles


```{r}
poisson_model = glm(art ~ 1 + kid5 + mar + ment,
                    family=poisson(link=log),data=bioChemists)
summary(poisson_model)
```

### Hypothesis test for goodness of fit

```{r}
p_value = pchisq(poisson_model$deviance,
                 poisson_model$df.residual, lower.tail = F)
print(p_value)
```


```{r}
p_value = pchisq(poisson_model$null.deviance-logit$deviance,
                poisson_model$df.null-logit$df.residual, lower.tail = F)
print(p_value)
```

```{r}
anova(poisson_model,test="Chisq")
```


\newpage

# Log-Linear Regression

Log-linear models allow us to model asscociation between between two or more variables in contingency table. In a log-linear model, there are no well defined explanatory/response variables. This is because we are focused more on the *interaction* between two variables.

## Contingency Table

Contingency table displays number of observations for a given combination of factors.

This definition is best represented by an example. 

```{r}
bioChemists$art_binary <- sapply(bioChemists$art,function(x) ifelse(x > 2, 1, 0))
bioChemists$ment_binary <- sapply(bioChemists$ment,function(x) ifelse(x > median(bioChemists$ment), 1, 0))

```

### One-Way Contingency Table
```{r}
table(art_relative=bioChemists$art_binary)
```

## Two-Way Contingency Table
```{r}
table(art_relative=bioChemists$art_binary,ment=bioChemists$ment_binary)
```

### Independent Model

```{r}
contigency_table = table(art_relative=bioChemists$art_binary,ment=bioChemists$ment_binary)
```

Need to convert contigency table in a form that is acceptable to `glm`
```{r}
contigency_table.df = as.data.frame(contigency_table)
```

```{r}
log_linear_model <- glm(Freq ~ art_relative + ment, 
            data = contigency_table.df, family = poisson)
```

```{r}
summary(log_linear_model)
```
### Saturated Model

```{r}
log_linear_model <- glm(Freq ~ art_relative*ment, 
            data = contigency_table.df, family = poisson)
```


```{r}
summary(log_linear_model)
```
```{r}
anova(log_linear_model,test='Chisq')
```

## Three-Way Contingency Table
```{r}
table(art_relative=bioChemists$art_binary,ment=bioChemists$ment_binary,
      kid5=bioChemists$kid5)
```

### Independent Model
```{r}
contigency_table = table(art_relative=bioChemists$art_binary,
                         ment=bioChemists$ment_binary,
                         kid5=bioChemists$kid5)
```

Need to convert contigency table in a form that is acceptable to `glm`
```{r}
contigency_table.df = as.data.frame(contigency_table)
```

```{r}
log_linear_model_int <- glm(Freq ~ art_relative + ment + kid5, 
            data = contigency_table.df, family = poisson)
```

```{r}
summary(log_linear_model_int)
```


### Saturated Model

```{r}
log_linear_model_sat <- glm(Freq ~ art_relative*ment*kid5, 
            data = contigency_table.df, family = poisson)
```


```{r}
summary(log_linear_model_sat)
```

```{r}
anova(log_linear_model_int,log_linear_model_sat,test='Chisq')
```

```{r}
anova(log_linear_model_sat,test='Chisq')
```

# Hierarchical modeling
```{r}
student_data <- read.csv("data/hsb1.csv")
school_data  <- read.csv("data/hsb2.csv")
student_data$ses_grandmean <- student_data$ses - mean(student_data$ses) # Grand-mean centered student SES 
school_data$sm_ses_grandmean <- school_data$meanses - mean(school_data$meanses) # Grand-mean centered school SES

data <- merge(student_data, school_data, by = "id")

ses_group_mean <- aggregate(data$ses, list(data$id), FUN = mean, data = data) # Group-mean centered student SES
names(ses_group_mean)<- c('id','groupmeanSES')
data <- merge(data, ses_group_mean, by = "id")

groups <- unique(data$id)[sample(1:160,20)]
subset <- data[data$id%in%groups, ]

xyplot(mathach ~ ses | as.factor(id), subset,
       col.line = 'black',
       type = c("p", "r"),
       main = 'Variability in Math Achievement ~ SES Relationship')

xyplot(mathach ~ ses |as.factor(id), subset,
                        col.line = 'black',
                        type = c("p", "smooth"),
                        main = 'Variability in Math Achievement ~ SES Relationship')

xyplot(mathach ~ ses, subset,
       type = c("p", "smooth"),
       group = data$id,
       main = 'Variability in Math Achievement ~ SES Relationship')

unconditional <- lmer(mathach ~ 1 + (1|id), data = data)
summary(unconditional) # on p-values in nlme: https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html
confint(unconditional) # you can also just calculate an approximate 95% confidence interval yourself: estimate +/- 2(SE) (in this case: 12.64 +/- 2(0.24))
unconditional_2 <- lme(mathach ~ 1, random = ~ 1 | id, data = data)
summary(unconditional_2)

random_intercept_fixed_slope <- lmer(mathach ~ 1 + groupmeanSES + (1|id), data = data)
summary(random_intercept_fixed_slope)
confint(random_intercept_fixed_slope)
random_intercept_fixed_slope_2 <- lme(mathach ~ 1 + groupmeanSES, random = ~ 1 | id, data = data)
summary(random_intercept_fixed_slope_2)

random_intercept_random_slope <- lmer(mathach ~ 1 + groupmeanSES + (1 + groupmeanSES|id), data = data)
summary(random_intercept_random_slope)
random_intercept_random_slope_2 <- lme(mathach ~ 1 + groupmeanSES, random = ~ 1 + groupmeanSES | id, data = data)
summary(random_intercept_random_slope_2)

fixed_intercept_random_slope <- lmer(mathach ~ 1 + groupmeanSES + (0 + groupmeanSES|id), data = data)
summary(fixed_intercept_random_slope)
fixed_intercept_random_slope_2 <- lme(mathach ~ 1 + groupmeanSES, random = ~ 0 + groupmeanSES | id, data = data)
summary(fixed_intercept_random_slope_2)

fixed_slope_level_two_variable <- lmer(mathach ~ 1 + groupmeanSES + sm_ses_grandmean + (1|id), data = data)
summary(fixed_slope_level_two_variable)
fixed_slope_level_two_variable_2 <- lme(mathach ~ 1 + groupmeanSES + sm_ses_grandmean, random = ~ 1 | id, data = data)
summary(fixed_slope_level_two_variable_2)

random_slope_level_two_variable <- lmer(mathach ~ 1 + groupmeanSES + sm_ses_grandmean + (1 + groupmeanSES|id), data = data)
summary(random_slope_level_two_variable)
random_slope_level_two_variable_2 <- lme(mathach ~ 1 + groupmeanSES + sm_ses_grandmean, random = ~ 1 + groupmeanSES | id, data = data)
summary(random_slope_level_two_variable_2)  

fixed_slope_cl_interaction <- lmer(mathach ~ 1 + groupmeanSES*sm_ses_grandmean + (1|id), data = data)
summary(fixed_slope_cl_interaction)
fixed_slope_cl_interaction_2 <- lme(mathach ~ 1 + groupmeanSES*sm_ses_grandmean, random = ~ 1 | id, data = data)
summary(fixed_slope_cl_interaction_2)

random_slope_cl_interaction <- lmer(mathach ~ 1 + groupmeanSES*sm_ses_grandmean + (1 + groupmeanSES|id), data = data)
summary(random_slope_cl_interaction)
random_slope_cl_interaction_2 <- lme(mathach ~ 1 + groupmeanSES*sm_ses_grandmean, random = ~ 1 + groupmeanSES | id, data = data)
summary(random_slope_cl_interaction_2)

logit_random_intercept_and_slope <- glmer(minority ~ groupmeanSES + (1 + groupmeanSES | id), data = data, 
                                    family = binomial(link="logit"))
summary(logit_random_intercept_and_slope)

specified_variance_covariance_matrix_for_random_effects <- lme(mathach ~ 1 + groupmeanSES*sm_ses_grandmean, random = ~ 1 + groupmeanSES | id, 
                                                               correlation = corAR1(), data = data) # just an example, not needed in this case (useful for growth curve models)!
summary(specified_variance_covariance_matrix_for_random_effects)
```