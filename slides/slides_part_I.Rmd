---
title: "Part I: Exploratory Data Analysis, Linear Regression, ANOVA"
output:
  beamer_presentation 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = 'pdf')
```


# Load Packages 

First, we must load the packages that will be used in the first part of this workshop.


```{r, message=FALSE, warning=FALSE }
library(pastecs, quietly = TRUE)
library(lm.beta,  quietly = TRUE)
library(lmtest,  quietly = TRUE)
library(foreign,  quietly = TRUE)
library(lattice,  quietly = TRUE)
library(lme4,  quietly = TRUE)
library(nlme,  quietly = TRUE)
library(survival,  quietly = TRUE)
library(dplyr,  quietly = TRUE)
library(ggfortify,  quietly = TRUE)
library(survminer,  quietly = TRUE)
library(rms,  quietly = TRUE)
```
---



# Exploratory Data Analysis
## Basic Statistical Analysis 
### Data set description

In this section, we will be using the iris data set. This data set contains measurement data of the flower of certain plant species. The data set has five variables:

- *Sepal.Length* - measurements of Sepal length
- *Sepal.Width* - measurements of Sepal width
- *Petal.Length* - measurements of Petal length
- *Petal.Width* - measurements of Petal width
- *Species* - species of the plant

```{r}
data = iris
```
---


## Basic Statistical Analysis 
### Univariate descriptive statistics

The function, `stat.desc`, can be used to do a statistical analysis of data. It returns the mean, median, maximum, minimum, etc of a data set.

\tiny
```{r}
stat.desc(data$Sepal.Width)
```
\normalsize

---

## Basic Statistical Analysis 
### Descriptive statistics by groups 

Using `tapply`, we compute the same descriptive statistics above but with grouped species of the same sepal width. `tapply` takes

- *first argument*: the input data to which we will apply the statistical function 
- *second argument*: the grouping data which tells the statistical function how to group the input data
- *third argument*: the statistical function.

We will be considering the statistical functions: `mean`,`sd` and `length`.


---


## Basic Statistical Analysis 
### Descriptive statistics by groups 

We will be considering the statistical functions: `mean`,`sd` and `length`.

\tiny
```{r}
mean <- tapply(data$Sepal.Length, data$Sepal.Width, mean)
standard_deviation <- tapply(data$Sepal.Length, data$Sepal.Width, sd)
number_of_observations <- tapply(data$Sepal.Length, data$Sepal.Width, length)
head(round(cbind(mean, standard_deviation, number_of_observations), digits = 6))
```
\normalsize


---

## Basic Data Visualization
### Histogram

It is important to get an idea of the "structure" of your data. To do this, we can use histograms. We use the `hist` function to plot a distribution of sepal width.

```{r,out.width = '50%'}
hist(data$Sepal.Width,
     xlab='Sepal Width',ylab='Count',
     main= 'Histogram plot of sepal length vs sepal width')
```


---

### Scatter Plot

We use the `plot` function to create a scatter plot of sepal length vs sepal width.

```{r,out.width = '50%'}
plot(data$Sepal.Length, data$Sepal.Width,
     xlab='Sepal length', ylab='Sepal Width',
     main= 'Scatter plot of sepal length vs sepal width')
```

---

## Basic Data Visualization
### Stacking plots (without ggplot2)

We also differentiate the scatter plot above by species. We plot each species separately. 

First, we call `plot` to create a canvas with an set of points associated with Setosa. If we were to call `plot` again, this would clear the previous plot.

--- 

## Basic Data Visualization
### Stacking plots (without ggplot2)

Rather, we call `points` to add scatter plots to the existing plot. `points` does not clear the previous plot.
 
\tiny
```{r,out.width = '40%'}
plot(data[data$Species == "setosa", ]$Sepal.Length, 
     data[data$Species == "setosa", ]$Sepal.Width, 
     xlab = 'Sepal Length',  ylab= 'Sepal Width',
     xlim = range(as.matrix(data$Sepal.Length)), 
     ylim = range(as.matrix(data$Sepal.Width)),
     main= 'Scatter plot of sepal length vs sepal width')

points(data[data$Species == "versicolor", ]$Sepal.Length,
       data[data$Species == "versicolor", ]$Sepal.Width,
       col = 'red')

points(data[data$Species == "virginica", ]$Sepal.Length,
       data[data$Species == "virginica", ]$Sepal.Width, col = 'blue')
```
\normalsize


---


## Basic Data Visualization
### Scatter plot matrix

It is quite cumbersome to call `plot` multiple times to create scatter plots for various pairs of explanatory variables. 

There exists a convenience function, `pairs`, that will create a matrix of scatter plots for all possible explanatory variable combinations.

We give the `pairs` the first four columns of `data`. It will create a scatter plot matrix for sepal length, sepal width, petal length, and petal width.


---

### Scatter plot matrix

```{r,out.width = '70%'}
pairs(data[,c(1:4)])
```


---


## Basic Data Visualization
### Correlation matrix

From the scatter plot matrix above, we can see the qualitative correlation patterns between explanatory variables. We can also calculate these correlations explicitly and as a matrix using the `cor` function.
\tiny
```{r,out.width = '70%'}
cor(data[,c(1:4)])
```

\normalsize

---


# Linear regression

Given a response variable, $y$, explanatory variables, $X_i$, and assuming that

$$ y_i = \beta_0 +\beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_n X_{ni} +\varepsilon$$

where $\varepsilon$ is a noise term, linear regression attempts to find the coefficients $\beta_i$ that makes $\varepsilon$ as small as possible.

The formula above assumes that response variable is a linear function of explanatory variables. The noise term is added to account for the fact that most data is noisy and will not perfectly fit its 'true' function. 

Linear regression also assumes that the noise is normally distributed with zero mean. 


---

# Linear regression


If these two assumptions are violated then 
  $$r_i = y_i - \beta_0 +\beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_n X_{ni}$$
  
  and $\sum_{i=1}^m r_i^2$ is generally a large number. $\sum_{i=1}^m r_i^2$ is called the residual standard error.
  
  
---

## Linear regression syntax

To regress the response variable, `Sepal.Width`, with explanatory variables, `Sepal.Length`, `Species`, `Petal.Length` and `Petal.Width`, we use the `lm` function.

The first argument of `lm` is the formula. The name of the column of the response variable is written first. It is followed by a tilde, `~`. After the tilde, we write names of the explanatory variable each separated by a `+`. `1` can also be added to the formula to represent a constant term.


---


## Linear regression syntax
### General Syntax: Constant Term

First, I use the formula `Sepal.Width ~ 1`. This formula is equivalent to

$$\text{Sepal.Width} = \beta_0 + \varepsilon.$$

Note that constant term is simply the mean response variable.

---

## Linear regression syntax
### General Syntax: Constant Term


\tiny
```{r}
ols <- lm(Sepal.Width ~ 1, data = data)
summary(ols)
print(mean(data$Sepal.Width))
```
\normalsize

---


## Linear regression syntax
### General Syntax: Constant Term

Note that constant term is simply the mean response variable.


\tiny
```{r}
coef(ols)
print(mean(data$Sepal.Width))
```
\normalsize

---

## Linear regression syntax
### General Syntax: Constant Term

\tiny 

```{r,out.width = '70%'}
plot(data[data$Species == "setosa", ]$Sepal.Length, 
     data[data$Species == "setosa", ]$Sepal.Width, 
     xlab = 'Sepal Width',  ylab= 'Sepal Length',
     xlim = range(as.matrix(data$Sepal.Length)), 
     ylim = range(as.matrix(data$Sepal.Width)),
     main= 'Scatter plot of sepal length vs sepal width')

coefs = coef(ols)
abline(coefs[1],0)
```
\normalsize

---

## Linear regression syntax
### General Syntax: Explanatory Variable and Constant Term

I use then formula `Sepal.Width ~ 1 + Sepal.Length`. This formula is equivalent to

$$\text{Sepal.Width} = \beta_0 + \beta_1\text{Sepal.Length}+ \varepsilon$$

--- 

## Linear regression syntax
### General Syntax: Explanatory Variable and Constant Term

\tiny
```{r,out.width = '60%'}
ols <- lm(Sepal.Width ~ 1 + Sepal.Length, data = data)
summary(ols)
```
\normalsize

---

## Linear regression syntax
### General Syntax: Explanatory Variable and Constant Term

\tiny
```{r,out.width = '70%'}
plot(data[data$Species == "setosa", ]$Sepal.Length, 
     data[data$Species == "setosa", ]$Sepal.Width, 
     xlab = 'Sepal Width',  ylab= 'Sepal Length',
     xlim = range(as.matrix(data$Sepal.Length)), 
     ylim = range(as.matrix(data$Sepal.Width)),
     main= 'Scatter plot of sepal length vs sepal width')

coefs = coef(ols)
abline(coefs[1],coefs[2])
```
\normalsize

---

## Linear regression syntax
### General Syntax: Factors

I use then formula `Sepal.Width ~ 1 + Sepal.Length + as.factor(Species)`. This formula is equivalent to

$$\tiny{\text{Sepal.Width} = \beta_0 + \beta_1\text{Sepal.Length} + \beta_2 I(\text{Species} = \text{Veriscolor}) + \beta_3 I(\text{Species} = \text{Virginica}) + \varepsilon}$$

---

## Linear regression syntax
### General Syntax: Factors

\tiny
```{r,out.width = '50%'}
ols <- lm(Sepal.Width ~ 1 + Sepal.Length + as.factor(Species), data = data)
summary(ols)
```
\normalsize

---

## Linear regression syntax
### General Syntax: Factors

\tiny

```{r,fig.show='hide'}
plot(data[data$Species == "setosa", ]$Sepal.Length, 
     data[data$Species == "setosa", ]$Sepal.Width, 
     xlab = 'Sepal Width',  ylab= 'Sepal Length',
     xlim = range(as.matrix(data$Sepal.Length)), 
     ylim = range(as.matrix(data$Sepal.Width)),
     main= 'Scatter plot of sepal length vs sepal width')

points(data[data$Species == "versicolor", ]$Sepal.Length,
       data[data$Species == "versicolor", ]$Sepal.Width,
       col = 'yellow')

points(data[data$Species == "virginica", ]$Sepal.Length,
       data[data$Species == "virginica", ]$Sepal.Width, col = 'blue')
coefs = coef(ols)
abline(coefs[1],coefs[2])
abline(coefs[3] + coefs[1],coefs[2],col='yellow')
abline(coefs[4] +coefs[1],coefs[2],col='blue')
```
  
\normalsize


---

## Linear regression syntax
### General Syntax: Factors

\tiny
```{r,  echo=FALSE, results='hide'}
plot(data[data$Species == "setosa", ]$Sepal.Length, 
     data[data$Species == "setosa", ]$Sepal.Width, 
     xlab = 'Sepal Width',  ylab= 'Sepal Length',
     xlim = range(as.matrix(data$Sepal.Length)), 
     ylim = range(as.matrix(data$Sepal.Width)),
     main= 'Scatter plot of sepal length vs sepal width')

points(data[data$Species == "versicolor", ]$Sepal.Length,
       data[data$Species == "versicolor", ]$Sepal.Width,
       col = 'yellow')

points(data[data$Species == "virginica", ]$Sepal.Length,
       data[data$Species == "virginica", ]$Sepal.Width, col = 'blue')
coefs = coef(ols)
abline(coefs[1],coefs[2])
abline(coefs[3] + coefs[1],coefs[2],col='yellow')
abline(coefs[4] +coefs[1],coefs[2],col='blue')
```

\normalsize

---


## Linear regression syntax
### Advanced Syntax: Nonlinear Regression

The `lm` function can be extended to nonlinear functions. For example, it is possible include a quadratic term in our model.

$$\text{Sepal.Width} = \beta_1\text{Sepal.Length} + \beta_2\text{Sepal.Length}^2  + \varepsilon$$

To add a quadratic term to the model, add `I(Sepal.Length^2)` to the left side of the tilde, `~`. It is also possible to include higher order nonlinear terms, such as cubic, quintic, etc.


---


## Linear regression syntax
### Advanced Syntax: Nonlinear Regression

\tiny

```{r,out.width= '50%'}
ols_quadratic <- lm(Sepal.Width ~ Sepal.Length + I(Sepal.Length^2), data = data)
summary(ols_quadratic)
```

\normalsize

---


## Linear regression syntax
### Advanced Syntax: Interaction term

It is also possible to include model interaction between explanatory variables. 

$${\tiny \text{Sepal.Width} = \beta_1\text{Sepal.Length} + \beta_2\text{Petal.Length} + \beta_3 \text{Sepal.Length}\times\text{Petal.Length} + \varepsilon}$$
$\text{Sepal.Length}\times\text{Petal.Length}$ models simple interaction between Sepal.Length and Petal.Length.

---

## Linear regression syntax
### Advanced Syntax: Interaction term

\tiny


```{r,out.width= '50%'}
ols_interaction <- lm(Sepal.Width ~ Sepal.Length + Petal.Length
                      + Sepal.Length*Petal.Length, data = data)
summary(ols_interaction)
```


\normalsize


---

## Linear regression syntax
### Advanced Syntax: Interaction term

\tiny

Note that using the formula `Sepal.Width ~ Sepal.Length*Petal.Length` produces the same result as `Sepal.Length + Petal.Length + Sepal.Length*Petal.Length`.


```{r,out.width= '50%'}
ols_interaction <- lm(Sepal.Width ~ Sepal.Length*Petal.Length, data = data)
summary(ols_interaction)
```

\normalsize

---


## Linear regression syntax
### Advanced Syntax: Non-linear regression and Interaction term 


```{r,results='hide'}
ols_q_i <- lm(Sepal.Width ~ Sepal.Length*as.factor(Species) 
              + I(Sepal.Length^2), data = data)
summary(ols_q_i)
```


---

\tiny

```{r,echo=FALSE}
ols_q_i <- lm(Sepal.Width ~ Sepal.Length*as.factor(Species) + I(Sepal.Length^2), data = data)
summary(ols_q_i)
```

\normalsize

---

## Obtaining the residuals

Recall it is possible the measure the error between the model and response variables. Given least squares fit, 

$$r_i = y_i - \beta_0 +\beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_n X_{ni}$$
r is the residual vector and $\sum_{i=1}^m r_i^2$ is the residual standard error. 


---

## Obtaining the residuals

It is possible to obtain residuals from the output of `lm`

```{r,out.width= '50%'}
res <- ols$residuals
head(data.frame(res=res))
```


---


## Obtaining the fitted values

It is also possible to get predicted model values from the output of `lm`.

```{r}
pred <- ols$fitted.values
head(data.frame(pred=pred,orig=data$Sepal.Width ))
```

---


## Obtaining the ANOVA table

`anova` allows us to compare the variance in residuals to explained variance of sequentially built models. 

\small
```{r}
anova(ols)
```
\normalsize

---

## Obtaining the variance-covariance matrix

Covariance measures how well two variables vary together.

For an linear model, variance-covariance matrix is an array in which the 

-  $(i,j)$ entry is the covariance between the $i$th coefficent and the $j$ coefficient.
-  $(i,i)$ entry is the variance for the $i$th coefficent.

The `vcov` returns the variance-covariance matrix of a linear model.

```{r,results='hide'}
vcov(ols)
```

---


## Obtaining the variance-covariance matrix

\tiny
```{r,echo=FALSE}
library(knitr)
```

```{r}
vcov_mat <- vcov(ols)
```

```{r,echo=FALSE}
kable(vcov_mat)
```
\normalsize

---


## Obtaining confidence intervals for the coefficients

We can use the `confint` to return the confidence interval of the coefficients of our linear model. The `level` argument specifies the percentage of confidence.

```{r}
confint(ols, level = 0.95)
```

---

## Obtaining confidence interval of the mean response

We can use the `predict` to determine the predicted response values of data points from our linear model. 

`predict` has arguments:

- 1st argument: the model object
- 2nd argument: dataframe of data points
- `interval`: this is an optional argument. This is the type of interval calculation
- `level`: this is an optional argument. This specifies the percentage confidence.
- `se.fit`: this is an optional argument. It specifies if stamdard errors are required.

---

## Obtaining confidence interval of the mean response

Here, we take `interval="confidence"`.

\small
```{r}
predict(ols, data.frame(Sepal.Length=4.0, Species="setosa"), 
        interval="confidence", level = 0.95, se.fit=TRUE)
```
\normalsize

---

## Obtaining prediction limits for new observation

\tiny
Here, we take `interval="prediction"`.
```{r}
predict(ols, data.frame(Sepal.Length=4.0, Species="setosa"),  
        interval="prediction", level = 0.95, se.fit=TRUE)
```
\normalsize

---

## Obtaining confidence band for the entire regression line

```{r}
alpha = 0.05
n = dim(iris)[1]
ci <- predict(ols, data.frame(Sepal.Length=4.0,
                              Species="setosa"),
              interval="confidence", level= 1-alpha,
              se.fit=TRUE)
yh.hat <- ci$fit[1]
se.yh.hat <- ci$se.fit
w <- sqrt(2*qf(1-alpha, 2, n-2))
lower_bound <- yh.hat - w*se.yh.hat
upper_bound <- yh.hat + w*se.yh.hat
band <- c(lower_bound, upper_bound)
band
```

---


## Standardized regression

It is difficult to compare coefficients if the measurement units of the covariates differ widely. 

Standardization removes the scale of the covariates. From each covariate column, standardization subtracts the mean and divides by the standard deviation.

To standardize our model, we convert the unstandardize model to a standarduzed model using `lm.beta`. 

---


## Standardized regression

\tiny
```{r}
lm.beta(ols)
```
\normalsize

---

# Model selection

There are various statistical tests and estimators which can be used to compare models. In this workshop, we will be covering:

- F statistic
- AIC
- BIC
- adjusted $R^2$.

---

## General linear test approach

 The F test tells us if there is a statistically significant decrease in residual standard error.  We use a significance level $0.05$ of in this workshop.
 
 We use the `anova` function to conduct the F-test between pairs of models. Note that the `anova` function can take more than two models.

---

### Comparison of interaction model with and without quadratic term

Comparing the quadratic model with interaction, `ols_q_i`, and the model without the quadratic term, `ols_interaction`,

\tiny
```{r}
anova(ols_interaction,ols_q_i)
```
\normalsize

There is significant reduction in error after including the quadratic term.

---

### Comparison of quadratic model with and without interaction term

Comparing the quadratic model with interaction, `ols_q_i`, and the model without the interaction term, `ols_quadratic`,
\tiny
```{r}
anova(ols_quadratic,ols_q_i)
```
\normalsize

There is significant reduction in error after including the interaction term.

---


### Comparison of linear models with and without quadratic term

Comparing the quadratic model, `ols_quadratic`, and the model without the quadratic term, `ols`,

\tiny
```{r}
anova(ols_quadratic, ols)
```
\normalsize
There is significant reduction in error after including the quadratic term.


---



### Comparison of linear models with and without interaction term

Comparing the interaction model, `ols_interaction`, and the model without the interaction term, `ols`,

\tiny
```{r}
anova(ols,ols_interaction)
```
\normalsize

There is significant reduction in error after including the interaction term.

---


## AIC, BIC, and Adjusted $R^2$
```{r}
aic <- c(AIC(ols_quadratic), AIC(ols_interaction))
bic <- c(BIC(ols_quadratic), BIC(ols_interaction))
ar2 <- c(summary(ols_quadratic)$adj.r.squared,
         summary(ols_interaction)$adj.r.squared)
cbind(aic, bic, ar2)
```


---


## Step wise selection

To use stepwise selection, you need to define a base model without predictors and a full model with all possible predictors to be considered. 
If you want quadratic or interaction terms, you need to add them explicitely; otherwise, `.`adds all the variables in the dataset.

\tiny
```{r}
base <- lm(Sepal.Width ~ 1, data=data)
retailer_quadratic <- lm(Sepal.Width ~ . + I(Sepal.Length^2), data=data)
retailer_interaction <- lm(Sepal.Width ~ . + Sepal.Length*Species, data=data)
```
\normalsize

---

## Step wise selection

Specifying `direction = "both"` selects step wise selection. If you're not interested in looking at the steps taken by the algorithm, you can select `trace = FALSE`.

\small
```{r,results='hide'}
step_1 <- step(base, scope = list(upper=retailer_quadratic, 
                                  lower= ~1), 
               direction = "both",
               trace=TRUE)
```
\normalsize


---

## Step wise selection

\tiny
```{r,echo=FALSE}
step_1 <- step(base, scope = list(upper=retailer_quadratic, lower= ~1), direction = "both", trace=TRUE)
```
\normalsize

---

## Step wise selection

\small
```{r,results='hide'}
step_2 <- step(base, scope = list(upper=retailer_interaction,
                                  lower= ~1),
               direction = "both", 
               trace=TRUE)
```
\normalsize

---

## Step wise selection
\tiny
```{r,echo=FALSE}
step_2 <- step(base, scope = list(upper=retailer_interaction,
                                  lower= ~1),
               direction = "both", 
               trace=TRUE)
```
\normalsize

---


## Step wise selection

\tiny
```{r}
summary(step_1)
```
\normalsize

---

## Step wise selection

\tiny
```{r}
summary(step_2)
```
\normalsize

---


# Diagnostics

## Normal probability plot

Each residual is plotted against its expected value under normality. Near linearity suggests normality.

\tiny 

```{r,out.width='70%'}
std_res <- rstandard(ols_quadratic)
qqnorm(std_res, ylab="Standardized Residuals", xlab="Normal Scores") 
qqline(std_res)
```
\normalsize

---

## Residual plots

Under linearity and constant variance, this should appear as a random cloud of points centered at 0. As long as no residuals stand out from the others, the model fits all observations.

```{r,out.width='70%'}
plot(res ~ pred, xlab="Fitted", ylab="Residual",
     main="Residual plot against fitted values")
abline(h=0)
```

---

## Residual plots

Under linearity and constant variance, this should appear as a random cloud of points centered at 0.

```{r,out.width='70%'}
plot(res ~ data$Sepal.Length, xlab="X", 
     ylab="Residual", main="Residual plot against X")
abline(h=0)
```

---

## Test for multicollinearity

This procedure works for numerical variables. 

```{r}
# See also function vif() in package "car"
VIF <- 1/(1-summary(lm(Sepal.Length ~ as.factor(Species),
                       data = data))$r.squared)
VIF
```

---

## Test for heteroskedasticity

This is the Breusch-Pagan test.

```{r}
bptest(ols_quadratic, studentize = FALSE)
```

---


## Test for outlying Y observations--studentized deleted residual

This procedure follows the recommendations of Kutner et al. (2005), Applied Linear Statistical Models.

\tiny
```{r,out.height='55%'}
p <- 5 # numer of parameters
case <- c(1:n) # n defined above
plot(case, rstudent(ols_quadratic), type="l", xlab="Case Numbers", 
     ylab="Studentized Deleted Residuals", main="Test for Outlying Y Values")
text(case, rstudent(ols_quadratic), case)
alpha <- 0.05
crit <- qt(1-alpha/(2*n), n-p-1)
which(abs(rstudent(ols_quadratic)) >=crit ) # Here there's no evidence of outlying Y observations
```
\normalsize


---

## Test for outlying X observations--hat matrix leverage values


This procedure follows the recommendations of Kutner et al. (2005), Applied Linear Statistical Models.

\tiny
```{r,out.height='55%'}
leverage <- hatvalues(ols_quadratic)
plot(case, leverage, type="l", xlab="Case Numbers", 
     ylab="Leverage", main="Test for Outlying X Values")
text(case, leverage, case)
abline(h=0.5, col=2)
X_out <- which(leverage>0.5)
leverage[X_out] # Here there's no outlying X observations
```
\normalsize

---


## Tests for influential observations
### Cook's distance


Usually less than 10% or 20% indicates little influence on the fitted values. Near 50% or more indicates influence on the fit of the regression function.

\tiny
```{r,out.height='55%'}
plot(case, cooks.distance(ols_quadratic), type="l", xlab="Case Numbers", 
     ylab="Cook's Distance", main = "Test for Influential Values: Cook's Distance")
text(case, cooks.distance(ols_quadratic))
```
\normalsize

---

### Cook's distance


```{r}
inf_obs <- which(cooks.distance(ols_quadratic)>0.5)
inf_obs
```


---

### DFFITS


For small to medium-sized datasets, `abs(DFFITS)>1` indicates influence. The procedure below is for large datasets.

\tiny
```{r,out.height='60%'}
plot(case, dffits(ols_quadratic), type="l", xlab="Case Numbers", 
     ylab="DFFITS", main = "Test for Influential Values: DFFITS")
text(case, dffits(ols_quadratic))
```
\normalsize

---

### DFFITS

\tiny
```{r,out.height='60%'}
inf_obs2 <- which(abs(dffits(ols_quadratic))>2/sqrt(p/n))
inf_obs2
```
\normalsize


---

### DFBETAS

For small to medium-sized datasets, `abs(DFBETAS)>1` indicates influence. The procedure below is for large datasets.

\tiny
```{r,out.height='10%'}
inf_obs3 <- which(abs(dfbeta(ols_quadratic))>2/sqrt(n))
inf_obs3
```
\normalsize 
